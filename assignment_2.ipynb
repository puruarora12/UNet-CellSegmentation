{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6S7DHV3pqERC"
   },
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIJsXYmUp8qO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import PIL\n",
    "\n",
    "import random\n",
    "import wandb\n",
    "#import any other library you need below this line\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmOBtE8PqH4w"
   },
   "source": [
    "### Loading data\n",
    "\n",
    "Upload the data in zip format to Colab. Then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMLW_lgTqRcL"
   },
   "outputs": [],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UoM-TMIqTna"
   },
   "source": [
    "### Defining the Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6awOO200qYSZ"
   },
   "outputs": [],
   "source": [
    "class Cell_data(Dataset):\n",
    "    def __init__(self, data_dir, size, train=True, train_test_split=0.8, augment_data=True):\n",
    "        # ######################### inputs ##################################\n",
    "        # data_dir(string) - directory of the data#########################\n",
    "        # size(int) - size of the images you want to use###################\n",
    "        # train(boolean) - train data or test data#########################\n",
    "        # train_test_split(float) - the portion of the data for training###\n",
    "        # augment_data(boolean) - use data augmentation or not#############\n",
    "        super(Cell_data, self).__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.images = os.listdir(os.path.join(data_dir, \"scans\"))\n",
    "\n",
    "        self.train_set = []\n",
    "        end = int(len(self.images) * train_test_split)\n",
    "        for i in range(0, end):\n",
    "            self.train_set.append(self.images[i])\n",
    "        self.test_set = []\n",
    "        for i in range(end, len(self.images)):\n",
    "            self.test_set.append(self.images[i])\n",
    "\n",
    "        self.transforms1 = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.transforms2 = transforms.Compose([\n",
    "            transforms.PILToTensor(),\n",
    "        ])\n",
    "\n",
    "        self.isTrain = train\n",
    "        self.augment_data = augment_data\n",
    "\n",
    "        self.size = size\n",
    "        # initialize the data class\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.isTrain:\n",
    "            img_item = self.train_set[idx]\n",
    "        else:\n",
    "            img_item = self.test_set[idx]\n",
    "\n",
    "        # img = Image.open(os.path.join(self.data_dir,\"scans\",img_item)).convert(\"L\")\n",
    "        # label = Image.open(os.path.join(self.data_dir,\"labels\",img_item)).convert(\"1\")\n",
    "        img = Image.open(os.path.join(self.data_dir, \"scans\", img_item))\n",
    "        label = Image.open(os.path.join(self.data_dir, \"labels\", img_item))\n",
    "        # load image and mask from index idx of your data\n",
    "\n",
    "        img = img.resize((self.size, self.size))\n",
    "        label = label.resize((self.size, self.size))\n",
    "\n",
    "        # data augmentation part\n",
    "        if self.augment_data:\n",
    "            augment_mode = np.random.randint(0, 6)\n",
    "            if augment_mode == 0:\n",
    "                img = img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
    "                label = label.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
    "                # flip image vertically\n",
    "            elif augment_mode == 1:\n",
    "                img = img.transpose(PIL.Image.FLIP_TOP_BOTTOM)\n",
    "                label = label.transpose(PIL.Image.FLIP_TOP_BOTTOM)\n",
    "                # flip image horizontally\n",
    "            elif augment_mode == 2:\n",
    "                width, height = img.size\n",
    "                img = img.resize((width*2, height*2))\n",
    "                label = label.resize((width*2, height*2))\n",
    "\n",
    "                left = (width*2 - width)/2\n",
    "                top = (height*2 - height)/2\n",
    "                right = (width*2 + width)/2\n",
    "                bottom = (height * 2 + height) / 2\n",
    "\n",
    "                img = img.crop((left, top, right, bottom))\n",
    "                label = label.crop((left, top, right, bottom))\n",
    "                # zoom image\n",
    "            elif augment_mode == 3:\n",
    "                rand_gamma = np.random.uniform(0.0,1.5)\n",
    "                img = transforms.functional.adjust_gamma(img, gamma=rand_gamma)\n",
    "                # Gamma adjust\n",
    "            elif augment_mode == 4:\n",
    "                shear_angle = random.randint(-20,20)\n",
    "                img= TF.affine(img, angle=0, translate=(0,0), scale = 1.0, shear=shear_angle)\n",
    "                label = TF.affine(label, angle=0, translate=(0,0), scale = 1.0, shear=shear_angle)\n",
    "                #Sheer Transform\n",
    "            else:\n",
    "                angle = np.random.randint(0, 90)\n",
    "                img = img.rotate(angle)\n",
    "                label = label.rotate(angle)\n",
    "                # rotate image\n",
    "\n",
    "        img = self.transforms1(img)\n",
    "        label = self.transforms2(label)\n",
    "\n",
    "        mean, std = img.mean([1, 2]), img.std([1, 2])\n",
    "        self.normTrans = transforms.Compose([\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "\n",
    "        img = self.normTrans(img)\n",
    "\n",
    "        return img, label\n",
    "        # return image and mask in tensors\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.isTrain:\n",
    "            return len(self.train_set)\n",
    "        else:\n",
    "            return len(self.test_set)\n",
    "        # return len(self.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jo6kRDASsc5t"
   },
   "source": [
    "### Define the Model\n",
    "1. Define the Convolution blocks\n",
    "2. Define the down path\n",
    "3. Define the up path\n",
    "4. combine the down and up path to get the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class twoConvBlock(nn.Module):\n",
    "    def __init__(self, input, output):\n",
    "        super(twoConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input, output, kernel_size=(3, 3))\n",
    "        self.conv2 = nn.Conv2d(output, output, kernel_size=(3, 3))\n",
    "        self.norm = nn.BatchNorm2d(output)\n",
    "        self.relu = nn.ReLU()\n",
    "        # initialize the block\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.conv1(input)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm(out)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "        # implement the forward path\n",
    "\n",
    "class downStep(nn.Module):\n",
    "    def __init__(self, input, output):\n",
    "        super(downStep, self).__init__()\n",
    "        self.conv = twoConvBlock(input, output)\n",
    "        self.maxPooling = nn.MaxPool2d((2, 2), stride=2)\n",
    "        # initialize the down path\n",
    "\n",
    "    def forward(self, input):\n",
    "        copy_out = self.conv(input)\n",
    "        out = self.maxPooling(copy_out)\n",
    "        return out, copy_out\n",
    "        # implement the forward path\n",
    "\n",
    "class upStep(nn.Module):\n",
    "    def __init__(self, input):\n",
    "        super(upStep, self).__init__()\n",
    "        output = int(input/2)\n",
    "        self.upSampling = nn.ConvTranspose2d(input, output, kernel_size=(2, 2),stride=(2, 2))\n",
    "        self.conv = twoConvBlock(input, output)\n",
    "\n",
    "    def forward(self, input, copy_input):\n",
    "        out = self.upSampling(input)\n",
    "        _, _, h, w = out.size()\n",
    "        # _, _, h2, w2 = copy_input.size()\n",
    "        # left = int((w2 - w)/2)\n",
    "        # top = int((h2 - h)/2)\n",
    "        # copy = transforms.functional.crop(copy_input, top=top, left=left, height=h, width=w)\n",
    "\n",
    "        cropTrans = transforms.Compose([\n",
    "            transforms.CenterCrop((h, w)),\n",
    "        ])\n",
    "        copy = cropTrans(copy_input)\n",
    "\n",
    "        out = torch.cat((copy, out), dim=1)\n",
    "        out = self.conv(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.down1 = downStep(1,64)\n",
    "        self.down2 = downStep(64,128)\n",
    "        self.down3 = downStep(128,256)\n",
    "        self.down4 = downStep(256,512)\n",
    "        self.conv = twoConvBlock(512,1024)\n",
    "        self.up1 = upStep(1024)\n",
    "        self.up2 = upStep(512)\n",
    "        self.up3 = upStep(256)\n",
    "        self.up4 = upStep(128)\n",
    "        self.endConv = nn.Conv2d(64, 2, kernel_size=(1, 1))\n",
    "        \n",
    "\n",
    "    def forward(self, input):\n",
    "        out, copy_out1 = self.down1(input)\n",
    "        out, copy_out2 = self.down2(out)\n",
    "        out, copy_out3 = self.down3(out)\n",
    "        out, copy_out4 = self.down4(out)\n",
    "        out = self.conv(out)\n",
    "        out = self.up1(out, copy_out4)\n",
    "        out = self.up2(out, copy_out3)\n",
    "        out = self.up3(out, copy_out2)\n",
    "        out = self.up4(out, copy_out1)\n",
    "        out = self.endConv(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5-0LnQItdth"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmFg17HktfBW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Paramteres\n",
    "\n",
    "# learning rate\n",
    "lr = 0.001  # 0.005  # 1e-2\n",
    "# number of training epochs\n",
    "epoch_n = 1  # 20  # 30\n",
    "# input image-mask size\n",
    "image_size = 360  # 400  # 320  # 572\n",
    "# root directory of project\n",
    "root_dir = os.getcwd()\n",
    "# training batch size\n",
    "batch_size = 1  # 4\n",
    "# use checkpoint model for training\n",
    "load = False\n",
    "# use GPU for training\n",
    "gpu = True\n",
    "\n",
    "augment_data = True\n",
    "wandb.init( \n",
    "    project=\"UNet Cell Segmentation\", \n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": epoch_n,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"image_size\": image_size\n",
    "    }\n",
    ")\n",
    "\n",
    "data_dir = os.getcwd()+ '/data/cells'\n",
    "\n",
    "trainset = Cell_data(data_dir=data_dir, size=image_size, augment_data=augment_data)\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testset = Cell_data(data_dir=data_dir, size=image_size, train=False, augment_data=False)\n",
    "testloader = DataLoader(testset, batch_size=batch_size)\n",
    "\n",
    "device = torch.device('cuda:0' if gpu else 'cpu')\n",
    "\n",
    "model = UNet().to('cuda:0').to(device)\n",
    "# print(model)\n",
    "if load:\n",
    "    print('loading model')\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.99, 0.999), weight_decay=0.0005)  # 0.99\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.99, weight_decay=0.0005)\n",
    "\n",
    "train_loss_log = []\n",
    "test_loss_log = []\n",
    "\n",
    "model.train()\n",
    "begin = time.time() \n",
    "for e in range(epoch_n):\n",
    "    epoch_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    model.train()\n",
    "    for i, data in enumerate(trainloader):\n",
    "        image, label = data\n",
    "\n",
    "        # image = image.unsqueeze(1).to(device)\n",
    "        image = image.to(device)\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        pred = model(image)\n",
    "        #print(pred.shape)\n",
    "        label = label.squeeze(1)\n",
    "\n",
    "        crop_x = (label.shape[1] - pred.shape[2]) // 2\n",
    "        crop_y = (label.shape[2] - pred.shape[3]) // 2\n",
    "\n",
    "        label = label[:, crop_x: label.shape[1] - crop_x, crop_y: label.shape[2] - crop_y]\n",
    "\n",
    "        loss = criterion(pred, label)\n",
    "        # print(loss)\n",
    "        total_train += label.shape[0] * label.shape[1] * label.shape[2]\n",
    "        \n",
    "        _, pred_labels_train = torch.max(pred, dim = 1)\n",
    "        correct_train += (pred_labels_train == label).sum().item()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # print('batch %d --- Loss: %.4f' % (i, loss.item() / batch_size))\n",
    "    t_acc = correct_train / total_train\n",
    "    train_loss = epoch_loss / trainset.__len__()\n",
    "    print('Epoch %d / %d --- Loss: %.4f' % (e + 1, epoch_n, epoch_loss / trainset.__len__()))\n",
    "    train_loss_log.append(epoch_loss / trainset.__len__())\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader):\n",
    "            image, label = data\n",
    "\n",
    "            # image = image.unsqueeze(1).to(device)\n",
    "            image = image.to(device)\n",
    "            label = label.long().to(device)\n",
    "\n",
    "            pred = model(image)\n",
    "\n",
    "            label = label.squeeze(1)\n",
    "\n",
    "            crop_x = (label.shape[1] - pred.shape[2]) // 2\n",
    "            crop_y = (label.shape[2] - pred.shape[3]) // 2\n",
    "\n",
    "            label = label[:, crop_x: label.shape[1] - crop_x, crop_y: label.shape[2] - crop_y]\n",
    "\n",
    "            loss = criterion(pred, label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, pred_labels = torch.max(pred, dim=1)\n",
    "\n",
    "            total += label.shape[0] * label.shape[1] * label.shape[2]\n",
    "            correct += (pred_labels == label).sum().item()\n",
    "        v_acc = correct/total\n",
    "        v_loss = total_loss/testset.__len__()\n",
    "        print('Accuracy: %.4f ---- Loss: %.4f' % (v_acc, v_loss))\n",
    "\n",
    "        test_loss_log.append(total_loss / testset.__len__())\n",
    "        if correct/total > 0.75:\n",
    "              torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        \n",
    "    wandb.log({'train_accuracy': t_acc,'time': time.time()-begin, 'training_loss': train_loss, 'validation_loss': v_loss})\n",
    "    wandb.watch(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT-64s70tyBw"
   },
   "source": [
    "### Testing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ko9zFomNuCfC"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "testset = Cell_data(data_dir=data_dir, size=572, train=False, augment_data=False)\n",
    "testloader = DataLoader(testset, batch_size=batch_size)\n",
    "\n",
    "output_masks = []\n",
    "output_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(testset.__len__()):\n",
    "        image, labels = testset.__getitem__(i)\n",
    "\n",
    "        # input_image = image.unsqueeze(0).unsqueeze(0).to(device)\n",
    "        input_image = image.unsqueeze(0).to(device)\n",
    "        pred = model(input_image)\n",
    "\n",
    "        labels = labels.squeeze(0)\n",
    "        output_mask = torch.max(pred, dim=1)[1].cpu().squeeze(0).numpy()\n",
    "\n",
    "        crop_x = (labels.shape[0] - output_mask.shape[0]) // 2\n",
    "        crop_y = (labels.shape[1] - output_mask.shape[1]) // 2\n",
    "        # labels = labels[crop_x: labels.shape[0] - crop_x, crop_y: labels.shape[1] - crop_y].numpy()\n",
    "        labels = labels[crop_x: labels.shape[0] - crop_x, crop_y: labels.shape[1] - crop_y]\n",
    "\n",
    "        labels = labels.numpy()\n",
    "        output_masks.append(output_mask)\n",
    "        output_labels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot usingplt plot train-test plot\n",
    "\n",
    "plt.plot(range(epoch_n), train_loss_log, 'g', label='Training Loss')\n",
    "plt.plot(range(epoch_n), test_loss_log, 'r', label='Testing Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('Train-Test Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OrV7k1GuFSA"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(testset.__len__(), 2, figsize = (20, 20))\n",
    "\n",
    "for i in range(testset.__len__()):\n",
    "  axes[i, 0].imshow(output_labels[i])\n",
    "  axes[i, 0].axis('off')\n",
    "  axes[i, 1].imshow(output_masks[i])\n",
    "  axes[i, 1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "UNet_FW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
